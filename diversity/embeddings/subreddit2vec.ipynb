{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KCIMFvtVsqF"
   },
   "source": [
    "# Subreddit2Vec Model for Community Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muEMpZMJVsqG"
   },
   "source": [
    "Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.\n",
    "\n",
    "The Continuous Skip-gram Model predicts words within a certain range before and after the current word in the same sentence. The model predicts the context (or neighbors) of a word, given the word itself. The context of a word can be represented through a set of skip-gram pairs of (target_word, context_word) where context_word appears in the neighboring context of target_word. The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. [Source](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb#scrollTo=gK1gN1jwkMpU)\n",
    "\n",
    "In this case, we use a Subreddit2Vec model to generate subreddit embeddings rather than word embeddings. We apply the Word2Vec algorithm on interaction data by treating subreddits as \"words\" and the users that comment on them as \"contexts\" - every instance of a user commenting in a subreddit then becomes a word-context (subreddit-user) pair. Then, Subreddits are similar if and only if many similar users have the time and interest to comment in them both. [Source](https://www.cs.toronto.edu/~ashton/pubs/cultural-dims2020.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecXSwN1vVsqH"
   },
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpHdOnDsVsqH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IibZZHlgVsqI"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/reddit_user_data_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7udG05YfVsqI",
    "outputId": "8336c9eb-87c9-4a19-9e86-627c736a8a0a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iftOIXNVsqI",
    "outputId": "243d0beb-dec9-4f07-dc63-84c60bcbaa57"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IGzTatjVsqJ",
    "outputId": "be5ed3c3-f290-4002-b770-f02eb4eb7c7f"
   },
   "outputs": [],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxfIk-MzVsqJ",
    "outputId": "03ac5d9f-8d37-4ea2-eb68-e4cb6a6301bb"
   },
   "outputs": [],
   "source": [
    "df['user'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_FcQH9KVsqJ"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lN6smoGHVsqK"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaK85uPfVsqK"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOsQUhdOVsqK"
   },
   "outputs": [],
   "source": [
    "word_tokens = df[\"subreddit\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmXAMPeRVsqK"
   },
   "outputs": [],
   "source": [
    "context_tokens = df[\"user\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ywz-JQeqVsqL"
   },
   "outputs": [],
   "source": [
    "tokens = context_tokens + word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFN1EdEtVsqL"
   },
   "source": [
    "Create a vocabulary to save mappings from tokens to integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tudiansBVsqL",
    "outputId": "d048a398-ec2b-4374-c1f1-3c44adb8fc2f"
   },
   "outputs": [],
   "source": [
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__LSUDSCVsqL"
   },
   "source": [
    "Create an inverse vocabulary to save mappings from integer indices to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkUhxVnzVsqL",
    "outputId": "a8fb04fc-2487-4e90-c7d3-e5566d8c7cff"
   },
   "outputs": [],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1EolfN5VsqM"
   },
   "source": [
    "## Negative sampling for one skip-gram subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1yXAOD7VsqM",
    "outputId": "c2e32cc6-5f1b-42eb-a577-6731c210ad25"
   },
   "outputs": [],
   "source": [
    "# Create tuple for each comment in a subreddit - (subreddit, commenter/user)\n",
    "df['pair'] = list(zip(df.subreddit, df.user))\n",
    "pairing = df['pair'].tolist()\n",
    "pairing[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTDZjF2mVsqM",
    "outputId": "ed2d27cb-99ed-40a9-8495-4095ac0b1349"
   },
   "outputs": [],
   "source": [
    "for target, context in df['pair'][:20]:\n",
    "    print(f\"({vocab[target]}, {vocab[context]}): ({target}, {context})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3S_-YR-VsqM"
   },
   "source": [
    "The skipgrams function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the tf.random.log_uniform_candidate_sampler function to sample num_ns number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y5mczepVsqN",
    "outputId": "260ba66a-cf8b-4103-8bd3-297f3417417a"
   },
   "outputs": [],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = vocab[df['pair'][0][0]], vocab[df['pair'][0][1]]\n",
    "print(target_word, context_word)\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(type(negative_sampling_candidates))\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_iivI7SVsqN"
   },
   "source": [
    "## Construct one Training Example\n",
    "\n",
    "For a given positive (target_subreddit, context_user) skip-gram, you now also have num_ns negative sampled context words that do not appear in the window size neighborhood of target_word. Batch the 1 positive context_word and num_ns negative context words into one tensor. This produces a set of positive skip-grams (labelled as 1) and negative samples (labelled as 0) for each target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-BkFTT_VsqN"
   },
   "outputs": [],
   "source": [
    "# Add a dimension so you can use concatenation (on the next step).\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "# Concat positive context word with negative sampled words.\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H93NOZgiVsqN",
    "outputId": "4aeadebd-09e8-4f17-b983-f9ac5ffdbe66"
   },
   "outputs": [],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djJkHRx0VsqO"
   },
   "outputs": [],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqe121DbVsqO"
   },
   "source": [
    "## Compile all steps into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy2z2GSmVsqO"
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples and vocabulary size.\n",
    "\n",
    "def generate_training_data(tokens, tuples, num_ns, vocab_size, vocab, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all tuples in dataset\n",
    "    # Generate positive skip-gram pairs for a tuple\n",
    "    target_words, context_words = [], []\n",
    "    for i in tuples:\n",
    "        target_words.append(vocab[i[0]])\n",
    "        context_words.append(vocab[i[1]])\n",
    "    \n",
    "    positive_skip_grams = list(zip(target_words, context_words))\n",
    "    \n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "        context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "        \n",
    "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "        # Build context and label vectors (for one target word)\n",
    "        negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "        context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "        label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "        # Append each element from the training example to global lists.\n",
    "        targets.append(target_word)\n",
    "        contexts.append(context)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9a4ow0jgVsqP",
    "outputId": "628bbadc-22a3-444f-fdf6-0770ed535e86"
   },
   "outputs": [],
   "source": [
    "targets, contexts, labels = generate_training_data(tokens, df['pair'], 4, vocab_size, vocab, SEED)\n",
    "print(len(targets), len(contexts), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV_t72bPVsqP"
   },
   "source": [
    "To perform efficient batching for the potentially large number of training examples, use the tf.data.Dataset API. After this step, you would have a tf.data.Dataset object of (target_subreddit, context_user), (label) elements to train your Word2Vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLQX5O7uVsqP"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AClh1BT5VsqQ"
   },
   "source": [
    "Add cache() and prefetch() to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhSHhPZzVsqQ"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19Zfps6cVsqQ"
   },
   "source": [
    "## Model and Training\n",
    "\n",
    "The Word2Vec model can be implemented as a classifier to distinguish between true context users from skip-grams and false context users obtained through negative sampling. You can perform a dot product between the embeddings of target subreddit and context users to obtain predictions for labels and compute loss against true labels in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSHsAekJVsqQ"
   },
   "source": [
    "Use the Keras Subclassing API to define your Word2Vec model with the following layers:\n",
    "\n",
    "target_embedding: A tf.keras.layers.Embedding layer which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are (vocab_size * embedding_dim).\n",
    "context_embedding: Another tf.keras.layers.Embedding layer which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in target_embedding, i.e. (vocab_size * embedding_dim).\n",
    "dots: A tf.keras.layers.Dot layer that computes the dot product of target and context embeddings from a training pair.\n",
    "flatten: A tf.keras.layers.Flatten layer to flatten the results of dots layer into logits.\n",
    "With the subclassed model, you can define the call() function that accepts (target, context) pairs which can then be passed into their corresponding embedding layer. Reshape the context_embedding to perform a dot product with target_embedding and return the flattened result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_z4WoaWVsqQ"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=num_ns+1)\n",
    "        self.dots = Dot(axes=(3, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = self.dots([context_emb, word_emb])\n",
    "        return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMeZRpKLVsqR"
   },
   "source": [
    "## Define loss function and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWKjyqidVsqR"
   },
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0o_L_23VsqR"
   },
   "source": [
    "It's time to build your model! Instantiate your Word2Vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the tf.keras.optimizers.Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpKCDfNwVsqR"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQsqU-sZVsqR"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzJVjA8qVsqR"
   },
   "source": [
    "Train the model with dataset prepared above for some number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fl-_jbzVsqS",
    "outputId": "c42c055d-a8a2-4687-fed6-c2b20ea7de05"
   },
   "outputs": [],
   "source": [
    "word2vec.fit(dataset, epochs=10, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE75arxiVsqS"
   },
   "source": [
    "## Embedding lookup and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzVk6XDoVsqS"
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slH_Tbt_VsqS"
   },
   "source": [
    "Download the vectors.tsv and metadata.tsv to analyze the obtained embeddings in the [Embedding Projector](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyX3M5VdVsqS"
   },
   "outputs": [],
   "source": [
    "out_v = io.open('../datasets/vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('../datasets/metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END OF NOTEBOOK ###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "subreddit2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
