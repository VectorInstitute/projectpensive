{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd15df0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW, DistilBertTokenizerFast, DistilBertForSequenceClassification, get_scheduler\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "dataset = load_dataset(\"civil_comments\")\n",
    "\n",
    "\n",
    "class CivilCommentsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Builds split instance of the `civil_comments` dataset: https://huggingface.co/datasets/civil_comments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def build_data_split(split, num_data_points):\n",
    "    print(f\"Generating {num_data_points} data points for {split} split...\", end=\"\", flush=True)\n",
    "\n",
    "    civil_idx = []\n",
    "    uncivil_idx = []\n",
    "    num_civil = num_data_points / 2\n",
    "    num_uncivil = num_data_points / 2\n",
    "\n",
    "    for i, data in enumerate(dataset[split]):\n",
    "        if data[\"toxicity\"] < 0.5 and num_civil > 0:\n",
    "            civil_idx.append(i)\n",
    "            num_civil -= 1\n",
    "        elif data[\"toxicity\"] > 0.5 and num_uncivil > 0:\n",
    "            uncivil_idx.append(i)\n",
    "            num_uncivil -= 1\n",
    "\n",
    "        if num_civil == 0 and num_uncivil == 0:\n",
    "            break\n",
    "\n",
    "    indexes = civil_idx + uncivil_idx\n",
    "    random.shuffle(indexes)\n",
    "    encodings = tokenizer(dataset[split][indexes][\"text\"], truncation=True, padding=True)\n",
    "    labels = dataset[split][indexes][\"toxicity\"]\n",
    "\n",
    "    print(\"done\")\n",
    "    return encodings, labels\n",
    "\n",
    "\n",
    "encodings, labels = build_data_split(\"train\", 500)\n",
    "train_dataset = CivilCommentsDataset(encodings, labels)\n",
    "encodings, labels = build_data_split(\"validation\", 500)\n",
    "val_dataset = CivilCommentsDataset(encodings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba207c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=1,\n",
    ")\n",
    "model.dropout.p = 0\n",
    "model.add_module(module=nn.Sigmoid(), name=\"sigmoid\")\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "eval_data_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_data_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "def eval_mod():\n",
    "    mse_mean = []\n",
    "    acc_mean = []\n",
    "    for batch in eval_data_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = outputs.logits\n",
    "\n",
    "        mse_mean.append(torch.mean(torch.square(outputs - labels)))\n",
    "        acc_mean.append(\n",
    "            torch.mean(torch.eq(outputs.transpose(0, 1) > 0.5, labels > 0.5).float())\n",
    "        )\n",
    "\n",
    "    return torch.mean(torch.stack(mse_mean)), torch.mean(torch.stack(acc_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f97920",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in train_data_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        losses.append(float(loss.data))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    mse_mean, accuracy_mean = eval_mod()\n",
    "    loss_mean = torch.mean(torch.tensor(losses))\n",
    "    print(f\" After epoch {epoch} | Train Loss: {loss_mean:.2f}, Val MSE: {mse_mean:.2f}, Val Accuracy: {accuracy_mean:.2f}\")\n",
    "    model.save_pretrained(f\"./results/checkpoints/epoch-{epoch}\")\n",
    "\n",
    "model.save_pretrained(\"./results/final_model\")\n",
    "print(\"\\nProgram complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e556f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
