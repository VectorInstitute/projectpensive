{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CivilCommentsDataset:\n",
    "    \"\"\"\n",
    "    Loads and processes the `civil_comments` dataset: https://huggingface.co/datasets/civil_comments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"Building dataset...\")\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.dataset = None\n",
    "\n",
    "        self.data_path = pathlib.Path(\"data\")\n",
    "\n",
    "        # Load/build data sets\n",
    "        self.train_data = self.load_or_generate_tf_dataset(\"train\")\n",
    "        self.val_data = self.load_or_generate_tf_dataset(\"validation\")\n",
    "        self.test_data = self.load_or_generate_tf_dataset(\"test\")\n",
    "\n",
    "    def load_or_generate_tf_dataset(self, split):\n",
    "        \"\"\"\n",
    "        Build dataset if not already done so, otherwise load it from disk\n",
    "        \"\"\"\n",
    "\n",
    "        if not pathlib.Path.exists(self.data_path / split):\n",
    "            if self.dataset is None:\n",
    "                self.dataset = load_dataset(\"civil_comments\")\n",
    "\n",
    "            print(f\"Building {split} data...\")\n",
    "\n",
    "            # Generate features and labels\n",
    "            # Only grabbing 100 items for now\n",
    "            encodings = self.tokenizer(self.dataset[split][0:100][\"text\"], truncation=True, padding=True)\n",
    "            features = {x: encodings[x] for x in self.tokenizer.model_input_names}\n",
    "            labels = self.dataset[split].remove_columns(\n",
    "                [\"text\", \"identity_attack\", \"insult\", \"obscene\", \"severe_toxicity\", \"sexual_explicit\", \"threat\"]\n",
    "            ).to_pandas().to_numpy()\n",
    "            labels = labels[0:100]\n",
    "\n",
    "            # Build dataset and save to disk\n",
    "            data = tf.data.Dataset.from_tensor_slices((\n",
    "                features,\n",
    "                labels\n",
    "            ))\n",
    "            tf.data.experimental.save(data, str(self.data_path / split))\n",
    "\n",
    "            return data\n",
    "        else:\n",
    "            return tf.data.experimental.load(str(self.data_path / split))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CivilityModel:\n",
    "    \"\"\"\n",
    "    Trains a civility classifier model, leveraging Hugging Face and TensorFlow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_labels=7):\n",
    "\n",
    "        \"\"\"\n",
    "        num_labels: Number of units in final dense layer of network. Defaults to 7 for the 7 categories of\n",
    "            incivility in the dataset. Set to 1 for a simple civil/uncivil classifier.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define model and dataset\n",
    "        self.dataset = CivilCommentsDataset()\n",
    "\n",
    "        if len(tf.config.list_physical_devices(\"GPU\")) > 1:\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            with strategy.scope():\n",
    "                self.model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                    'distilbert-base-uncased',\n",
    "                    num_labels=num_labels\n",
    "                )\n",
    "        else:\n",
    "            self.model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                'distilbert-base-uncased',\n",
    "                num_labels=num_labels\n",
    "            )\n",
    "\n",
    "        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"civility_model\",\n",
    "            save_weights_only=False,\n",
    "            monitor='accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True\n",
    "        )\n",
    "        \n",
    "        # Freeze layers\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer):\n",
    "                layer.trainable = False\n",
    "        self.model.summary()\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        loss = tf.keras.losses.MeanSquaredError()\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=\"accuracy\")\n",
    "\n",
    "    def train(self, epochs, batch_size=32):\n",
    "\n",
    "        print(\"Beginning train...\")\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.dataset.train_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=self.dataset.val_data,\n",
    "            callbacks=[self.model_checkpoint_callback]\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def test(self, batch_size=32):\n",
    "\n",
    "        print(\"Beginning evaluation\")\n",
    "        self.model.evaluate(\n",
    "           self.dataset.test_data,\n",
    "           batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    def predict(self, x, x_tokenized):\n",
    "\n",
    "        if x_tokenized:\n",
    "            return self.model.predict(x)\n",
    "        else:\n",
    "            x_token = self.dataset.tokenizer(x, truncation=True, padding=True)\n",
    "            return self.model.predict(x_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "civility_model = CivilityModel()\n",
    "history = civility_model.train(\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n",
    "civility_model.model.save(\"civility_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
